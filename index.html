<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css">
<title>Yang Yang</title>
<link rel="icon" type="image/x-icon" href="assets/icon/favicon.ico" />
</head>
<body>
<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="assets/images/portrait.jpeg" alt="Photo" width="200px" height="200px" />&nbsp;</td>
<td align="left"><p>
<h3>Yang Yang</h3>

Ph.D. <strike>Candidate</strike> <br /></p>
<p>Email: <a href="mailto:yang5276@umn.edu">yang5276 [at] umn.edu</a></p>
<p>
<p>
<a href="https://scholar.google.com/citations?user=xeR2744AAAAJ&hl=en"> <img src="assets/icon/google-scholar.png" height="40px"></a>
<a href="https://github.com/st2yang"> <img src="assets/icon/github.png" height="40px"></a>
<a href="https://www.linkedin.com/in/yang-yang-92b882133/"> <img src="assets/icon/linkedin.png" height="40px"></a>
</p>
</p>
<p></center></p>
</td></tr></table>
<h2>About me {<a href="assets/files/resume.pdf">Resume</a>, <a href="assets/files/cv.pdf">CV</a>}</h2>
<p>I am a Research Scientist at Meta AI. I obtained my Ph.D. in Computer Science from the <a href="https://twin-cities.umn.edu/">University of Minnesota</a> (UMN), advised by <a href="http://people.ece.umn.edu/~cchoi/">Prof. Changhyun Choi</a>. My Ph.D. research is in the areas of Robotics and Machine Learning.<br /></p>
<h2>Publications {<a href="https://scholar.google.com/citations?user=xeR2744AAAAJ&amp;hl=en">Google Scholar</a>}</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2203.08037"><b>Interactive Robotic Grasping with Attribute-Guided Disambiguation</b></a> <br />
<b>Yang Yang</b>, Xibai Lou, Changhyun Choi<br />
<i>IEEE International Conference on Robotics and Automation</i> (ICRA), 2022 <br />
[<a href="https://arxiv.org/pdf/2203.08037.pdf">PDF</a>][<a href="https://sites.google.com/umn.edu/attr-disam">Website</a>][<a href="https://youtu.be/6WKUItkGuUc">Video</a>][<a href="https://drive.google.com/file/d/1bJqZZFPxcMVvCzP4mm7SPL9-QiT3SBXl">Poster</a>] [<a href="https://icra2022.org/program/awards">Outstanding Student Paper</a>]</p>
</li>
<li><p><a href="https://arxiv.org/abs/2203.00875"><b>Learning Object Relations with Graph Neural Networks for Target-Driven Grasping in Dense Clutter</b></a> <br />
Xibai Lou, <b>Yang Yang</b>, Changhyun Choi<br />
<i>IEEE International Conference on Robotics and Automation</i> (ICRA), 2022 <br />
[<a href="https://arxiv.org/pdf/2203.00875.pdf">PDF</a>][<a href="https://sites.google.com/umn.edu/graph-grasping">Website</a>][<a href="https://youtu.be/YuPzc6Ktg8s">Video</a>]</p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.02271"><b>Attribute-Based Robotic Grasping with One-Grasp Adaptation</b></a> <br />
<b>Yang Yang</b>, Yuanhao Liu, Hengyue Liang, Xibai Lou, Changhyun Choi<br />
<i>IEEE International Conference on Robotics and Automation</i> (ICRA), 2021 <br />
[<a href="https://arxiv.org/pdf/2104.02271.pdf">PDF</a>][<a href="https://sites.google.com/umn.edu/attributes-grasping">Website</a>][<a href="https://youtu.be/3udRLj_0Leo">Video</a>]</p>
</li>
<li><p><a href="https://arxiv.org/abs/2104.00776"><b>Collision-Aware Target-Driven Object Grasping in Constrained Environments</b></a> <br />
Xibai Lou, <b>Yang Yang</b>, Changhyun Choi <br />
<i>IEEE International Conference on Robotics and Automation</i> (ICRA), 2021 <br />
[<a href="https://arxiv.org/pdf/2104.00776.pdf">PDF</a>][<a href="https://youtu.be/QLTM6UkZ-Dw">Video</a>]</p>
</li>
<li><p><a href="https://arxiv.org/abs/1910.03781"><b>Learning Visual Affordances with Target-Orientated Deep Q-Network to Grasp Objects by Harnessing Environmental Fixtures</b></a> <br />
Hengyue Liang, Xibai Lou, <b>Yang Yang</b>, Changhyun Choi <br />
<i>IEEE International Conference on Robotics and Automation</i> (ICRA), 2021 <br />
[<a href="https://arxiv.org/pdf/1910.03781.pdf">PDF</a>][<a href="https://sites.google.com/umn.edu/ki-dqn/">Website</a>][<a href="https://youtu.be/XzIyhoOr9zs">Video</a>]</p>
</li>
<li><p><a href="https://arxiv.org/abs/1909.04840"><b>A Deep Learning Approach to Grasping the Invisible</b></a> <br />
<b>Yang Yang</b>, Hengyue Liang, Changhyun Choi<br />
<i>IEEE Robotics and Automation Letters</i> (RA-L), 2020 <br />
[<a href="https://arxiv.org/pdf/1909.04840.pdf">PDF</a>][<a href="https://sites.google.com/umn.edu/grasping-invisible">Website</a>][<a href="https://github.com/choicelab/grasping-invisible">Code</a>][<a href="https://youtu.be/_jssCcP_oKg">Video</a>]</p>
</li>
<li><p><a href="https://arxiv.org/abs/1910.06404"><b>Learning to Generate 6-DoF Grasp Poses with Reachability Awareness</b></a> <br />
Xibai Lou, <b>Yang Yang</b>, Changhyun Choi <br />
<i>IEEE International Conference on Robotics and Automation</i> (ICRA), 2020 <br />
[<a href="https://arxiv.org/pdf/1910.06404.pdf">PDF</a>][<a href="https://sites.google.com/umn.edu/reachability-aware-grasping">Website</a>][<a href="https://youtu.be/E4Az6OacHt4">Video</a>]</p>
</li>
<li><p><a href="http://mars.cs.umn.edu/papers/ISRR19_0045_FI.pdf"><b>Attitude Tracking from a Camera and an Accelerometer on Gyro-less Devices</b></a> <br />
Tien Do, Leo Neira, <b>Yang Yang</b>, Stergios I. Roumeliotis <br />
<i>International Symposium on Robotics Research</i> (ISRR), 2019 <br />
[<a href="http://mars.cs.umn.edu/papers/ISRR19_0045_FI.pdf">PDF</a>]</p>
</li>
</ul>
<h2>Professional Experience</h2>
<ul>
<li><p>Meta AI, USA | March 2022 - present <br />
Research Scientist (Machine Learning)</p>
</li>
<li><p><a href="https://www.merl.com/research/computer-vision">Mitsubishi Electric Research Laboratories (MERL)</a>, USA | 2020 (May - Aug.) <br />
Research Intern (Computer Vision)</p>
</li>
<li><p>Google, USA | 2018 (Jul. - Aug.) <br />
Visual-Inertial System Engineer</p>
</li>
</ul>
<h2>Professional Services</h2>
<dl>
<dt>Conference Reviewer:</dt>
<dd><p>
AAAI Conference on Artificial Intelligence (AAAI) | 2022 <br />
IEEE International Conference on Robotics and Automation (ICRA) | 2020 - 2022 <br /></p></dd>
<dt>Journal Reviewer:</dt>
<dd><p>
IEEE Robotics and Automation Letters (RA-L) | 2020 - 2022 <br /></p></dd>
</dl>
<h2>Honors &amp; Awards</h2>
<ul>
<li><p><a href="https://icra2022.org/program/awards">Outstanding Student Paper Award</a>, ICRA | 2022</p>
</li>
<li><p><a href="https://rc.umn.edu/past-awards">UMII-MnDRIVE Graduate Fellowship</a>, UMN | 2021</p>
</li>
<li><p>Departmental Fellowship, UMN | 2016</p>
</li>
<li><p>National Scholarship (three consecutive times), Ministry of Education of P.R. China | 2012 - 2014</p>
</li>
</ul>
<h2>Teaching</h2>
<ul>
<li><p>Teaching Assistant, <a href="https://www-users.cs.umn.edu/~hspark/csci5561_F2020/csci5561.html">Computer Vision</a>, UMN | Fall 2020</p>
</li>
<li><p>Teaching Assistant, Program Design and Development (C++), UMN | Spring 2018</p>
</li>
</ul>
</div>
<script src="https://cdn.bootcss.com/jquery/3.2.1/jquery.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdn.bootcss.com/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.bootcss.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
</script>
</body>
</html>
